---
title: "CapstoneProject"
author: "Hans M. Rupp"
date: "26 5 2022"
output: pdf_document
---

# Introduction

The aim of this project is to predict ratings for movies in the open source MovieLens dataset.
This is inspired by a 2006 competition by Netflix to improve its recommondation system. 
This project is part of the capstone project of HarvardX PH125.9x Data Science. 
It is based on https://rafalab.github.io/dsbook/large-datasets.html#recommendation-systems
The main benchmark is achieving a RMSE < 0.86490 on the validation data set.

**Recommender systems** can divided into two categories [1]

The **content filtering** approach creates a profile for each user or product. Some profiles are manually curated.

**Collaborative filtering** relies only on past user behavior.

The two primary areas of collaborative filtering are the
neighborhood methods and latent factor models
A product's neighbors are other products that get similar reatings by the same user.
Latent factor models explain the ratings by factors characterizing both items and users.
We will use a latent factor model here.

```{r include=FALSE}
library(tidyverse)
library(caret)
library(gridExtra)
```




# Loading the data

TODO: Replace later with code from project site
```{r}
load(file="data/edx")
load(file="data/validation")
load(file="data/movies")
```

# Data Analysis and Cleanup

The data structure of the dataset:
```{r}
str(edx)

```

```{r echo=FALSE}
edx_rows <- nrow(edx)
edx_cols <- ncol(edx)
val_rows <- nrow(validation)
val_cols <- ncol(validation)
```

The edx dataset contains `r edx_rows` observations (rows) with `r edx_cols` features (columns). It will be used for training the model.
The validation dataset contains `r val_rows` observations (rows) with `r val_cols` features (columns). It will be used for validating the model

**Checking for missing values**
```{r}
nas <- function(x) { any(is.na(x)) }
edx %>% summarise_all(nas)
validation %>% summarise_all(nas)
```
There are no columns with NAs in the edx or the validation data. So no clean-up is necessary

**Overall distribution of the ratings**

```{r}
edx %>% ggplot(aes(x=rating)) + 
  geom_histogram(binwidth = 0.5) + 
  scale_x_continuous(breaks = seq(min(edx$rating), max(edx$rating), by = 0.5))

```

\newpage
The **different genres** are
```{r}
dist_genres <- edx %>% distinct(genres)
dg <- as.vector(str_split(dist_genres$genres, "\\|", simplify = T))
unique(dg[dg != ""])
```


Distribution of ratings for  different genres

```{r}
hist_drama <- edx %>% filter(str_detect(genres, "Drama")) %>%
  ggplot(aes(x=rating)) + geom_histogram(binwidth = 0.5) + 
  scale_x_continuous(breaks = seq(0.5, 5, by = 1)) + labs(title="Drama")

hist_comedy <- edx %>% filter(str_detect(genres, "Comedy"))  %>%
  ggplot(aes(x=rating)) + geom_histogram(binwidth = 0.5) + 
  scale_x_continuous(breaks = seq(0.5, 5, by = 1)) + labs(title="Comedy")

hist_thriller <- edx %>% filter(str_detect(genres, "Thriller"))  %>% 
  ggplot(aes(x=rating)) + geom_histogram(binwidth = 0.5) +
  scale_x_continuous(breaks = seq(0.5, 5, by = 1)) + labs(title="Thriller")

hist_romance <- edx %>% filter(str_detect(genres, "Romance"))  %>%
  ggplot(aes(x=rating)) + geom_histogram(binwidth = 0.5) + 
  scale_x_continuous(breaks = seq(0.5, 5, by = 1)) + labs(title="Romance")

grid.arrange(hist_drama, hist_comedy, hist_thriller,hist_romance, ncol = 2)
```

\newpage
Different **users** have widely different **numbers of ratings**

```{r  message=FALSE, fig.width=5, fig.height=3}
edx %>% count(userId) %>% ggplot(aes(x=n)) + geom_histogram() + scale_x_log10() + 
  xlab("Ratings per user (log)")
```

Different **users** have different **rating means**

```{r message=FALSE, fig.width=5, fig.height=3 }
edx %>% group_by(userId) %>% summarise(user_mean=mean(rating)) %>% ggplot(aes(x=user_mean)) +
  geom_histogram()  + xlab("Average ratings per user")
```

\newpage
Different **movies** have different **numbers of ratings** 

```{r message=FALSE, fig.width=5, fig.height=3 }
edx %>% group_by(movieId) %>% summarise(n=n()) %>% ggplot(aes(x=n)) + geom_histogram() +
  scale_x_log10() + xlab("Ratings per movie (log)")

```

Different **movies** have different **rating means**

```{r message=FALSE, fig.width=5, fig.height=3 }
edx %>% group_by(movieId) %>% summarise(movie_mean = mean(rating)) %>% ggplot(aes(x=movie_mean)) +
  geom_histogram() + xlab("Average rating per movie")

```
 

\newpage

**Release year** has some influence on **average rating**

Extract the release date from the title

```{r}
release_year_edx <- as.numeric(str_sub(edx$title, start=-5, end=-2))
edx <- edx %>% mutate(year=release_year_edx)
validation <- validation %>% mutate(year=as.numeric(str_sub(title, start=-5, end=-2)))
```
Plot of the average rating for each release year

```{r ,message=FALSE, fig.width=5, fig.height=3 }
edx %>% group_by(year) %>% summarise(avg_rating = mean(rating)) %>%
  ggplot(aes(x=year, y=avg_rating)) + 
  geom_point() + geom_smooth()

```


\newpage
# Matrix Factorization
We will use Matrix Factorization to build a model to predict movie ratings

movie _i_
user _u_
year _y_
genre _g_


## RMSE
The Root Mean Square Error will be used as the Loss function

RMSE = $\sqrt{\frac{1}{N}\sum_{u,i,y,g} (\hat{y}_{u,i,y,g}-y_{u,i,y,g})^2}$
```{r}

RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

## Naive model: simply using the mean ratings
This model assumes the same rating $\mu$ for all movies i and users u with an error $\epsilon _{u,i}$  
Y~u,i~ = $\mu + \epsilon _{u,i}$

```{r}
mu_rating <- mean(edx$rating)
rmse_mean <- RMSE(validation$rating, mu_rating)
rmse_mean

```
This gives a RMSE of `r rmse_mean`

We build a table of the RMSEs for the different models:

```{r}
models <- tibble(model="Mean", rmse=rmse_mean)
```

## Effect of different movies
As we saw above different movies are rated differently.

We can add the term b~i~ for the average ranking for movie i

$Y_{u,i} = \mu + b_{i} + \epsilon_{u,i}$

Theoretically we could calculate b~i~ using linear regression. But with this amount of data that would be very costly.

Instead we can just use $b_{i} = Y_{u,i} - \mu$

Calculate the average rating b_i for each movie minus the overall average rating mu_rating = `r mu_rating`:

```{r}

movie_avgs <- edx %>% group_by(movieId) %>% summarise(b_i = mean(rating - mu_rating))
```
Plot them:

```{r ,message=FALSE, fig.width=5, fig.height=3}

movie_avgs %>% ggplot(aes(b_i)) + geom_histogram(bins = 10)
```

Join the movie_avg data.frame to the edx (training) and validation set:

```{r}

edx_movies <- edx %>% left_join(movie_avgs, by="movieId")
validation_movies <- validation %>% left_join(movie_avgs, by="movieId")
```
Calculating the predicted ratings $Y_{u,i} = \mu + b_{i}$

```{r}
predicted_ratings_movies <- mu_rating + validation_movies$b_i
```


Calculating the new RMSE

```{r}
rmse_movies <- RMSE(predicted_ratings_movies, validation$rating)
models <- rbind(models, c("movies", rmse_movies))
models
```

The RMSE has improved to `r models$rmse[2]` 

## Effect of different users
As we saw above different users have different rating means.

We can add the term b~u~ for the average rating for user u

$Y_{u,i} = \mu + b_{i} + b_{u} + \epsilon_{u,i}$

Calculate the average rating b_u for each movie minus the overall average rating mu_rating and b_i:

```{r}
user_avg <- edx_movies %>% group_by(userId) %>% summarise(b_u = mean(rating - mu_rating - b_i)) 
```

Join the user_avg data.frame to the edx_movies (training) and validation_movies set:

```{r}
edx_movies_users <- edx_movies %>% left_join(user_avg, by="userId")
validation_movies_users <- validation_movies %>% left_join(user_avg, by="userId")
```

Plot the distribution of user effects b_u

```{r ,message=FALSE, fig.width=5, fig.height=3}

edx_movies_users %>% ggplot(aes(x=b_u)) + geom_histogram()
```

Calculating the predicted ratings $Y_{u,i} = \mu + b_{i} + b_{u}$

```{r}

predicted_ratings_movies_users <- mu_rating + validation_movies_users$b_i + validation_movies_users$b_u
rmse_movies_users <- RMSE(predicted_ratings_movies_users, validation$rating)
models <- rbind(models, c("movies users", rmse_movies_users))
models
```

The RMSE has improved to `r models$rmse[3]` 

## Effect of the release year

We can add the term b~y~ for the average rating for movies released in year y

$Y_{u,i,y} = \mu + b_{i} + b_{u} + b_{y} + \epsilon_{u,i}$

Calculate the average rating b_u for each movie minus the overall average rating mu_rating and b_i and b_u:

```{r}
year_avg <- edx_movies_users %>% group_by(year) %>% summarise(b_y = mean(rating - mu_rating - b_i - b_u))

```

Plot the effect of the release year b_y

```{r, fig.width=5, fig.height=3}
year_avg %>% ggplot(aes(x=b_y)) + geom_histogram()
```

Join the year_avg data.frame to the edx_movies_users (training) and validation_movies_users set:

```{r}
edx_movies_users_years <- edx_movies_users %>% left_join(year_avg, by="year")
validation_movies_users_years <- validation_movies_users %>% left_join(year_avg, by="year")
```

Calculating the predicted ratings $Y_{u,i} = \mu + b_{i} + b_{u} + b_{y}$

```{r}
predicted_ratings_movies_users_years <- mu_rating + validation_movies_users_years$b_i + validation_movies_users_years$b_u + validation_movies_users_years$b_y

rmse_movies_users_years <- RMSE(predicted_ratings_movies_users_years, validation$rating)
rmse_movies_users_years

models <- rbind(models, c("movies users years", rmse_movies_users_years))
knitr::kable(models)
```

The RMSE has only marginally improved to `r models$rmse[4]`

## Effect of the genre

We can add the term b~g~ for the average rating for movies of genre g

$Y_{u,i,y,g} = \mu + b_{i} + b_{u} + b_{y} + b_{g} + \epsilon_{u,i}$

Calculate the average rating b_g for each movie minus the overall average rating mu_rating and b_i and b_u and b_y:

```{r}
genre_avg <- edx_movies_users_years %>% 
  group_by(genres) %>% summarise(b_g = mean(rating - mu_rating - b_i - b_u - b_y))
```

Plot the effect of the genre b_g

```{r ,message=FALSE, fig.width=5, fig.height=3 }
genre_avg %>% ggplot(aes(x=b_g)) + geom_histogram()

```

Join the genre_avg data.frame to the edx_movies_users_year (training) and validation_movies_users_year set:

```{r}
edx_movies_users_years_genres <- edx_movies_users_years %>% 
  left_join(genre_avg, by="genres")
validation_movies_users_years_genres <- validation_movies_users_years %>%
  left_join(genre_avg, by="genres")

```

Calculating the predicted ratings $Y_{u,i} = \mu + b_{i} + b_{u} + b_{y} +b_{g}$
and the RMSE

```{r}
predicted_ratings_movies_users_years_genres <- mu_rating +
  validation_movies_users_years_genres$b_i + validation_movies_users_years_genres$b_u  +
  validation_movies_users_years_genres$b_y +  validation_movies_users_years_genres$b_g

rmse_movies_users_years_genres <- RMSE(predicted_ratings_movies_users_years_genres, validation$rating)

```

This model has improved the RMS to `r rmse_movies_users_years_genres`

```{r}
models <- rbind(models, c("movies users years genres", rmse_movies_users_years_genres))
knitr::kable(models)
```

## Regularization

We have seen that some movies have only a few ratings. Those can have a disproportionate effect on variability. Regularization shrinks the coefficient estimates for small samples towards zero. We introduce a penalty term $\lambda$. A greater $\lambda$ shrinks the coefficient more.

For example for the movie effect:

$\hat{b}_{i} (\lambda)= \frac{1} {\lambda + n_{i}} \sum_{u=2}^{n_{i}}(Y_{u,i} - \hat{\mu})$

If the sample size n~g~ is large, then the penalty $\lambda$ is effectively ignored since
$n_{i} + \lambda \approx n_{i}$

$\lambda$ is a tuning parameter. We can use cross-validation to choose it.

Choosing an optimal $\lambda$

```{r}

test_lambdas <- function(l) {
  movie_avgs_r <- edx %>% group_by(movieId) %>% summarise(b_i = sum(rating - mu_rating) /(n()+l))
  edx_movies_r <- edx %>% left_join(movie_avgs_r, by="movieId")
  validation_movies_r <- validation %>% left_join(movie_avgs_r, by="movieId")
  
  user_avg_r <- edx_movies_r %>% group_by(userId) %>% 
    summarise(b_u = sum(rating - mu_rating - b_i)/(n()+l))
  edx_movies_users_r <- edx_movies_r %>% left_join(user_avg_r, by="userId")
  validation_movies_users_r <- validation_movies_r %>% left_join(user_avg_r, by="userId")
  
  year_avg_r <- edx_movies_users_r %>% group_by(year) %>% 
    summarise(b_y = sum(rating - mu_rating - b_i - b_u)/(n()+l))
  edx_movies_users_years_r <- edx_movies_users_r %>% left_join(year_avg_r, by="year")
  validation_movies_users_years_r <- validation_movies_users_r %>% 
    left_join(year_avg_r, by="year")
  
  genre_avg_r <- edx_movies_users_years_r %>% group_by(genres) %>% 
    summarise(b_g = sum(rating - mu_rating - b_i - b_u - b_y)/(n()+l))
  edx_movies_users_years_genres_r <- edx_movies_users_years_r %>% 
    left_join(genre_avg_r, by="genres")
  validation_movies_users_years_genres_r <- validation_movies_users_years_r %>%
    left_join(genre_avg_r, by="genres")
  
  predictions <- mu_rating + validation_movies_users_years_genres_r$b_i +
    validation_movies_users_years_genres_r$b_u +
    validation_movies_users_years_genres_r$b_y +
    validation_movies_users_years_genres_r$b_g
  rmse_movies_users_years_genres_reg <- RMSE(predictions, validation$rating)
  rmse_movies_users_years_genres_reg
}

lambdas <- seq(0,10, 1)
rmses <- sapply(lambdas, test_lambdas)

dfr <- data.frame(lambdas, rmses)
dfr %>% ggplot(aes(x=lambdas, y=rmses)) + geom_point() + labs(x="Lambda", y="RMSE")
```

Using the optimal $\lambda$ gives the final model

```{r}
l <- lambdas[which.min(rmses)]

movie_avgs_r <- edx %>% group_by(movieId) %>% summarise(b_i = sum(rating - mu_rating) /(n()+l))
edx_movies_r <- edx %>% left_join(movie_avgs_r, by="movieId")
validation_movies_r <- validation %>% left_join(movie_avgs_r, by="movieId")

user_avg_r <- edx_movies_r %>% group_by(userId) %>% 
  summarise(b_u = sum(rating - mu_rating - b_i)/(n()+l))
edx_movies_users_r <- edx_movies_r %>% left_join(user_avg_r, by="userId")
validation_movies_users_r <- validation_movies_r %>% left_join(user_avg_r, by="userId")

year_avg_r <- edx_movies_users_r %>% group_by(year) %>% 
  summarise(b_y = sum(rating - mu_rating - b_i - b_u)/(n()+l))
edx_movies_users_years_r <- edx_movies_users_r %>% 
  left_join(year_avg_r, by="year")
validation_movies_users_years_r <- validation_movies_users_r %>% left_join(year_avg_r, by="year")

genre_avg_r <- edx_movies_users_years_r %>% group_by(genres) %>% 
  summarise(b_g = sum(rating - mu_rating - b_i - b_u - b_y)/(n()+l))
edx_movies_users_years_genres_r <- edx_movies_users_years_r %>% 
  left_join(genre_avg_r, by="genres")
validation_movies_users_years_genres_r <- validation_movies_users_years_r %>%
  left_join(genre_avg_r, by="genres")

predictions <- mu_rating + validation_movies_users_years_genres_r$b_i +
  validation_movies_users_years_genres_r$b_u +
  validation_movies_users_years_genres_r$b_y + 
  validation_movies_users_years_genres_r$b_g
rmse_movies_users_years_genres_reg <- RMSE(predictions, validation$rating)
rmse_movies_users_years_genres_reg

models <- rbind(models, c("movies users years genres regular", rmse_movies_users_years_genres_reg))
```

Comparing the different models:

```{r}
knitr::kable(models)
```
# Conclusion
The final model has a RMSE of `r rmse_movies_users_years_genres_reg` < 0.86490. So we have achieved our goal.



[1] https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf